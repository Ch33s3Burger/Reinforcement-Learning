{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17994b78-6a6d-4c3e-989e-eca5c51a69f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44345f1620904ff13359ae76b8bc9a91",
     "grade": false,
     "grade_id": "cell-cf260d06fae1d219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "![DSME-logo](./images/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Orignal Paper: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
    "\n",
    "Additional References:\n",
    "1. OpenAI SpinningUp:\n",
    "    * [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html#exploration-vs-exploitation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b4678-24de-4ad0-9f1e-9eceb8948132",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e0ab18763a7d8ef602e704ec31a39b2",
     "grade": false,
     "grade_id": "cell-504ce4176d6e9291",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# DSME Bonus Point Assignment II Part PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade911a4-09db-44d2-8ad2-01ff06ae8340",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "438e8880c7aa8ad48915fb0a4665a1ab",
     "grade": false,
     "grade_id": "cell-761d2bc7f1d71a94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7cfa7-401d-4072-9d7d-3c83239240ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0654c1140317598dd784e652ebb5f0b",
     "grade": false,
     "grade_id": "cell-d94fa13233a1224c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'ppo.ipynb'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99a26-0f9b-4388-9d43-56c1d3da771c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d616801c28a47ca01acf1feb888b0dd1",
     "grade": false,
     "grade_id": "cell-e623b76307b63117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634a991-dcd4-497d-bbdc-3c658264094a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "447019ea538e53a231bce3fcf0fa3dc2",
     "grade": false,
     "grade_id": "cell-d41085caf8268bd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Experiment\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment being run. Although initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45388b46-8aee-4cd5-954f-44bf7da7a231",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eba8d8753eb192c3e4afd5a5cb814fed",
     "grade": false,
     "grade_id": "cell-977de4c269d796db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'PPO'  # algorithm name, in this case it should be 'PPO'\n",
    "exp.env_id = 'LunarLander-v2'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "exp.random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = 42\n",
    "\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if exp.random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = exp.random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d6f0a-f868-4d36-8c90-8339f4951bc9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9bc680c4fcba59baf0eba961c3e3ad3",
     "grade": false,
     "grade_id": "cell-b1a70f2afa145e5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Rollout Buffer\n",
    "\n",
    "The second dictionary, `hypp`, is initialized in the following cell. It has keys and values containing the hyperparameters necessary to the algorithm. Similar to the `exp` dict, new keys and values are added to the `hypp` in the later sections. \n",
    "\n",
    "Define both `exp.num_envs` and `hypp.num_steps`.\n",
    "\n",
    "Initialize the multiple environments and run them in parallel using the `SyncVectorEnv` class from the gym library [More info: [Link](https://www.gymlibrary.dev/content/vectorising/)]. \n",
    "\n",
    "Next, create a rollout buffer based on the number of parallel envs `exp.num_envs` and the number of steps per env `hypp.num_steps`. It is later used to save episode trajectories during agent training. The buffer gets replaced with new trajectories at the beginning of every iteration of the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16605d0-d732-4f21-9aa1-15a6b56475e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1f0eb98664d4273277c0f52547a2cb5",
     "grade": false,
     "grade_id": "cell-6f6550a448f75441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypp = edict()\n",
    "\n",
    "exp.num_envs = 16  # number of parallel game environments\n",
    "hypp.num_steps = 1024  # number of steps to run in each environment per policy rollout\n",
    "\n",
    "# Intialize vectorized gym env\n",
    "envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "\n",
    "# RollOut Buffer Init\n",
    "observations = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "rewards = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "dones = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "values = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ea0ee-73e1-4661-a6ee-9e8a8efecbdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea6ab2adb3cc1f2fd2e65d4cdc21db0",
     "grade": false,
     "grade_id": "cell-6e7024cdc65825f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The `Agent` class consists of a deep MLP value function called the `critic`, and a deep MLP policy called the `actor`, both learned during training. \n",
    "\n",
    "The class has three methods:\n",
    "1. `get_value` evaluates the `critic` with a given observation (state) to obtain the learned estimate of the observation's value.\n",
    "2. `get_action` returns an action. The method first evaluates the `actor` to obtain the un-normalized probabilities of the actions, which is then used to create a categorical distribution over the actions. If `greedy = True` it returns the action with the highest probability else an action sampled from the distribution.\n",
    "3. `get_action_and_value`, when given an observation, returns the action sampled from the probability distribution (if `action=None`), the respective action's log probability, the entropy of the probability distribution, and the estimate of the observation's value according to the `critic` network.\n",
    "\n",
    "The `actor` and `critic` networks in the `Agent` class make use of the `layer_init` function to implement ideas derived from [Engstrom, Ilyas, et al., (2020)](https://openreview.net/forum?id=r1etN1rtPB), and [Andrychowicz et al. (2021)](https://openreview.net/forum?id=nIAxjsniDzg). The former introduces the idea of orthogonal initialization of weights for tanh activation. In the latter paper, decision C57 shows that initializing the policy such that the action distribution is centered as zero gives better performance. This is done by initializing the `actor`'s output layer weights with 0.01 std. (see the final layer of `self.actor`)\n",
    "\n",
    "Note: observation and state mean the same in the context of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14e9ac-852c-4470-ad10-470326d8dea9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0438a0b1c49ba3b2b5290261d023bb97",
     "grade": false,
     "grade_id": "cell-7297c656ba7ac8af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        logits = self.actor(x)\n",
    "        distribution = Categorical(logits=logits)\n",
    "        action = distribution.sample() if not greedy else distribution.mode\n",
    "        return action\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        distr = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = distr.sample()\n",
    "        return action, distr.log_prob(action), distr.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab02eab-03a8-4619-ae05-4ce61ccc743f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8190a13d755db84e1a6d50e906f844f",
     "grade": false,
     "grade_id": "cell-1eee5dbe0a8d3474",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - Stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by default to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.5.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "    - Quick reminder:  the `num_steps` key in the `hypp` dict is also a hyperparameter defined in Env & Rollout Buffer Init Section.\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd13f3c-ed78-4819-a324-c9d5cf492830",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9450557639182dd344878716d96cd8ff",
     "grade": false,
     "grade_id": "cell-8595f3f320f35239",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = True  # disable to speed up training\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = True  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 50\n",
    "exp.device = device.type\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent training specific parameters and hyperparameters\n",
    "hypp.total_timesteps = 1000000 # the training duration in number of time steps\n",
    "hypp.num_minibatches = 256  # number of minibatches for gradient updates\n",
    "hypp.update_epochs = 4  # epochs of updates after collecting one trajectory\n",
    "\n",
    "hypp.batch_size = int(exp.num_envs * hypp.num_steps)  # len of the rollout buffer\n",
    "hypp.minibatch_size = int(hypp.batch_size // hypp.num_minibatches)  # rollout buffer size / minibatch count\n",
    "\n",
    "hypp.learning_rate = 3e-4 #learning rate of gradient update step\n",
    "hypp.anneal_lr = True  # when True reduces the learning rate as the training progresses\n",
    "hypp.gamma = 0.999 # discount factor over future rewards\n",
    "hypp.norm_adv = False #whether to normalize the advantages to zero mean unit variance. If implemented set it to True.\n",
    "hypp.clip_coef = 0.2  # the clip coeff of the surrogate PPO-clip objective\n",
    "hypp.clip_vloss = False\n",
    "hypp.ent_coef = 0.01  # weight of the entropy term in the final loss function. Trade-off between exploration and exploitation\n",
    "hypp.vf_coef = 0.5  # weight of the value term in the final loss function\n",
    "hypp.max_grad_norm = 0.5  # the max norm of the gradients computed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b8f44-e272-4561-a122-69a61e27ff73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e0b8f7528a597128f6a224ead0e5c63",
     "grade": false,
     "grade_id": "cell-76424e99b814a13c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    " ## PPO helper functions (8 Points)\n",
    "\n",
    "In the following cells there are a few functions that are used during the PPO algorithm:\n",
    "1. `compute_advantage_estimates` computes the advantage estimates for the explored trajectories\n",
    "2. `compute_gae` computes the generalized advantage estimates for the explored trajectories\n",
    "3. `normalize_advantages` normalizes the given tensor to zero mean and unit variance\n",
    "4. `compute_policy_objective` computes the policy objective that is maximized to improve the agent\n",
    "5. `compute_clipped_value_loss` computes an alternative formulation for the value function loss\n",
    "\n",
    "For simpler notation, we assume that (only) the hyperparameter edict `hypp` is known to the functions and its values can be accessed as if it was given as a function parameter.\n",
    "\n",
    "Find and correct mistakes or implement missing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a250a0d-6b98-46ab-8c5f-5b949c2cd466",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1da12fa0576e6bffcde526de1a948549",
     "grade": false,
     "grade_id": "cell-b7aa157f3eae5086",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_advantage_estimates(rewards, values, dones, next_value, next_done):\n",
    "    '''\n",
    "    Computes the estimated advantages from given rewards and estimated values\n",
    "    :param rewards: accumulated rewards during the rollout (shape [num_steps, num_envs])\n",
    "    :param values: estimated values of traversed states (shape [nums_steps, num_envs])\n",
    "    :param next_value: bootstrapped value of the next state after rollout (shape [1, num_envs])\n",
    "    :param next_done: whether environment is finished in next state after rollout (shape [1, num_envs])\n",
    "    :return: the estimated advantages of policy (shape [num_steps, num_envs])\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        for t in reversed(range(hypp.num_steps)):\n",
    "            if t == hypp.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                returns[t] = rewards[t] + hypp.gamma * next_value * nextnonterminal\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                returns[t] = rewards[t] + hypp.gamma * returns[t+1] * nextnonterminal\n",
    "        advantages = returns - values\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09698270-aa9d-4284-91bf-35d749308bed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5512f3af0759458bee44a82acfd2725",
     "grade": false,
     "grade_id": "cell-440c0a19b93eabda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, next_value, next_done, gae_lambda):\n",
    "    '''\n",
    "    Computes the generalized advantage estimates(GAE) for every timestep from given rewards and estimated values\n",
    "    :param rewards: accumulated rewards during the rollout (shape [num_steps, num_envs])\n",
    "    :param values: estimated values of traversed states (shape [num_steps, num_envs])\n",
    "    :param next_value: bootstrapped value of the next state after rollout (shape [1, num_envs])\n",
    "    :param next_done: whether environment is finished in next state after rollout (shape [1, num_envs])\n",
    "    :param gae_lambda: scalar coefficient for gae computation\n",
    "    :return: generalized advantage estimates of trajectory (shape [num_steps, num_envs])\n",
    "    '''\n",
    "    # TODO: Part b)\n",
    "    # with torch.no_grad():\n",
    "    # ...\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fbb95-c83d-468e-8a41-ee49e0f34d46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e1ba40aee7ecc28b27709fec136020",
     "grade": false,
     "grade_id": "cell-f82fe417fda9cd30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small unittest for verification of implementation of GAE\n",
    "# simulates 2 environments run for 5 steps\n",
    "\n",
    "rewards_test = torch.tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1, 1.]]).transpose(0, 1)\n",
    "dones_test = torch.tensor([[0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.]]).transpose(0, 1)\n",
    "values_test = torch.tensor([[3., 2., 10., 8., 6.], [0., 1., 2., 1., 1.]]).transpose(0, 1)\n",
    "next_value_test = torch.tensor([[5., 10.]])\n",
    "next_done_test = torch.tensor([[0., 1.]])\n",
    "gae_lamba_test = 0.9\n",
    "\n",
    "# changing relevant hypp values for the test, saving old values\n",
    "gamma = hypp.gamma\n",
    "num_steps = hypp.num_steps\n",
    "hypp.gamma = 0.95\n",
    "hypp.num_steps = 5\n",
    "\n",
    "try:\n",
    "    test_adv = compute_gae(rewards_test, values_test, dones_test, next_value_test, next_done_test, gae_lamba_test)\n",
    "    exp_adv = torch.tensor([[-0.955, -1, -2.69425625, -1.51375, -0.25], [1.95, 0., 0.76225, 0.95, 0]]).transpose(0,1)\n",
    "    assert torch.allclose(test_adv, exp_adv), f\"Computed advantages incorrect.\\nExpected:\\n {exp_adv} \\nReceived:\\n {test_adv}\"\n",
    "    print(\"Test ok\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# resetting old values again\n",
    "hypp.gamma = gamma\n",
    "hypp.num_steps = num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c2c52-f22c-45da-9fe4-0b2cf1ccebae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "507754c63f14e8f4baa663e4ee0c84d3",
     "grade": false,
     "grade_id": "cell-358ad2b889db7e95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_advantages(advantages):\n",
    "    '''\n",
    "    Takes tensor of advantages and normalizes them to zero mean and unit variance\n",
    "    :param advantages: tensor of advantages to normalize (shape [n])\n",
    "    :return: tensor of normalized advantages (shape [n])\n",
    "    '''\n",
    "    # TODO: Part b)\n",
    "    # ...\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ace15-a2e2-457b-a3f2-b34fb5c68423",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d44864c4559a043bb6f2162be3cf753f",
     "grade": false,
     "grade_id": "cell-b876914b0bd75461",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_policy_objective(advantages, logprob_old, logprob):\n",
    "    '''\n",
    "    Computes the policy objective that is being optimized in the gradient step.\n",
    "    :param advantages: tensor of advantages (shape [n])\n",
    "    :param logprob_old: tensor of log-probabilites of actions of sampled policy (shape [n])\n",
    "    :param logprob: tensor of log-probabilites of actions of new policy (shape [n])\n",
    "    :return: objective function for policy (shape [1])\n",
    "    '''\n",
    "    # computing the probability ratio\n",
    "    logratio = logprob_old - logprob\n",
    "    ratio = logratio.exp()\n",
    "\n",
    "    # clipping\n",
    "    pg_loss1 = advantages * ratio\n",
    "    pg_loss2 = advantages * torch.clamp(ratio, 1 - hypp.clip_coef, 1 + hypp.clip_coef)\n",
    "    return torch.max(pg_loss1, pg_loss2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d97a1-b683-4f91-8e28-5311738485b7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a472139114778cf08e2496c87a697f7",
     "grade": false,
     "grade_id": "cell-2b85896fda50d416",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_clipped_value_loss(returns, old_values, values):\n",
    "    '''\n",
    "    Applies the same idea of trust region optimization to the value function,\n",
    "    constructing a clipped value loss.\n",
    "    :param returns: batch of returns collected in the trajectory (shape [n])\n",
    "    :param old_values: batch of value approximations of old policy (shape [n])\n",
    "    :param values: batch of value approximations of updated policy (shape [n])\n",
    "    :return the clipped value loss (shape [1])\n",
    "    '''\n",
    "    # TODO: Part b)\n",
    "    # ...\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352911d-e63b-4f32-819b-da2b3c03c996",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff591e819cc43c8bba9b2126d89c8250",
     "grade": false,
     "grade_id": "cell-1a89af41383dbe14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Training the Agent\n",
    "\n",
    "Before we begin training the agent we first initalize the logging (based on the repsective flags in the `exp` dict), object of the `Agent` class and the optimizer, followed by an inital set of observations. \n",
    "\n",
    "\n",
    "After that comes the main training loop which is comprised of:  \n",
    "1. learning rate annealing, \n",
    "2. collecting trajectories, i.e. filling the rollout buffer with transitions performed by the actor $\\pi_{\\theta_{old}}$\n",
    "3. computing the advantage estimates $\\hat{A}_t$ and returns $G_t$ of the trajectories in the rollout buffer\n",
    "4. performing several update epochs in minibatches on the obtained data by\n",
    "    1. Computing the simplified PPO-Clip Objective (surrogate PPO-Clip Objective) $$ L^{CLIP}(\\theta) = \\frac{1}{n}\\sum_{t=1}^n \\min\\left( r_t(\\theta) \\hat{A}_t, \\text{ clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right)$$ where $r_t(\\theta)$ is the action probability ratio at timestep $t$ $$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$$\n",
    "    2. Computing the value function loss $$ L^{VAL}(\\theta) = \\frac{1}{n}\\sum_{t=1}^{n}\\frac{1}{2}((V_{\\theta}(s_t) - G_t))^2 $$\n",
    "    3. Computing the mean entropy $$ L^{ENT}(\\theta) = \\frac{1}{n}\\sum_{t=1}^{n} H_t(\\pi_{\\theta})$$\n",
    "    4. Performing a gradient update w.r.t. the combined loss function $$ L(\\theta) = -L^{CLIP} + c_1 * L^{VAL} - c_2 * L^{ENT}$$ where $c_1$ and $c_2$ are weighting coefficients of the different terms. This optimization simultaneously tries to increase policy performance, decrease the value function approximation error, and increase entropy (to support exploration).\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "\n",
    "Note: we have two vectorised gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training. Vectorized environments automatically reset when the episode is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88473a21-85de-4763-bb16-cb902633a968",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1435c54066bfca5bdcbfa3103cdd3811",
     "grade": true,
     "grade_id": "cell-f4d34c2d641fc7d7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change.\n",
    "_grading_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38282990-7a5b-4d41-8dc0-4cf42aeeebf1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8abad8e9851c49f209f1fad132ff5e75",
     "grade": false,
     "grade_id": "cell-5ed0cde4d4a8eb38",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------ RUN INIT - DO NOT EDIT ---------------------- #\n",
    "envs.close()\n",
    "def train():\n",
    "    if _grading_mode: #DO NOT EDIT\n",
    "        return #DO NOT EDIT\n",
    "    # reinit run_name\n",
    "    exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "    # Init tensorboard logging and wandb logging\n",
    "    writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "    # create two vectorized envs: one to fill the rollout buffer with trajectories, and\n",
    "    # another to evaluate the agent performance at different stages of training\n",
    "    # Note: vectorized environments reset automatically once the episode is finished\n",
    "    envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "    envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.eval_count)])\n",
    "\n",
    "    # init list to track agent's performance throughout training\n",
    "    tracked_returns_over_training = []\n",
    "    tracked_episode_len_over_training = []\n",
    "    tracked_episode_count = []\n",
    "    last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated\n",
    "    greedy_evaluation = True  # whether to perform the evaluation in a greedy way or not\n",
    "    eval_max_return = -float('inf')\n",
    "\n",
    "    # Init observation to start learning\n",
    "    start_time = time.time()\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    done = torch.zeros(exp.num_envs).to(device)\n",
    "    num_updates = int(hypp.total_timesteps // hypp.batch_size)\n",
    "\n",
    "    pbar = notebook.tqdm(range(1, num_updates + 1))\n",
    "\n",
    "    # ------------------------- END RUN INIT --------------------------- #\n",
    "\n",
    "    global_step = 0\n",
    "    episode_step = 0\n",
    "    gradient_step = 0\n",
    "\n",
    "    # Create Agent class Instance and network optimizer\n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=hypp.learning_rate, eps=1e-5)\n",
    "\n",
    "    # training loop\n",
    "    for update in pbar:\n",
    "        # annealing the rate if true\n",
    "        if hypp.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * hypp.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        agent.eval()\n",
    "\n",
    "        # collect trajectories\n",
    "        for step in range(0, hypp.num_steps):\n",
    "            observations[step] = obs\n",
    "            dones[step] = done\n",
    "\n",
    "            # sample action and collect value from learned agent policy and value networks\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # execute the game and log data\n",
    "            next_obs, reward, done, infos = envs.step(action.cpu().numpy())\n",
    "\n",
    "            for idx, info in enumerate(infos):\n",
    "                # bootstrap value of the observation when done is true and the episode is truncated\n",
    "                if (\n",
    "                    done[idx]\n",
    "                    and info.get(\"terminal_observation\") is not None\n",
    "                    and info.get(\"TimeLimit.truncated\", False)\n",
    "                   ):\n",
    "                    terminal_obs = torch.tensor(info[\"terminal_observation\"]).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        terminal_value = agent.get_value(terminal_obs)\n",
    "                    reward[idx] += hypp.gamma * terminal_value\n",
    "\n",
    "                # log episode return and length to tensorboard\n",
    "                if \"episode\" in info.keys():\n",
    "                    episode_step += 1\n",
    "                    pbar.set_description(f\"global_step={global_step}, episodic_return={info['episode']['r']:.3f}\")\n",
    "                    writer.add_scalar(\"rollout/episodic_return\", info[\"episode\"][\"r\"], global_step+idx)\n",
    "                    writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], global_step+idx)\n",
    "                    writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "                    writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "            global_step += 1 * exp.num_envs\n",
    "            obs = next_obs\n",
    "\n",
    "    # -------------------------- EVALUATION - DO NOT EDIT ----------------------- #\n",
    "\n",
    "            # generate average performance statistics of current learned agent\n",
    "            if exp.eval_agent and episode_step % exp.eval_frequency == 0 and last_evaluated_episode != episode_step:\n",
    "                last_evaluated_episode = episode_step\n",
    "                tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)\n",
    "                tracked_returns_over_training.append(tracked_return)\n",
    "                tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "                tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "                # if there has been improvment of the model -\n",
    "                if np.mean(tracked_return) > eval_max_return:\n",
    "                    eval_max_return = np.mean(tracked_return)\n",
    "                    # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "                    hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "    # ------------------------------- END EVALUATION ---------------------------- #\n",
    "\n",
    "        next_done = done.reshape(1, -1)\n",
    "        agent.train()\n",
    "\n",
    "        # calculate advantages and returns\n",
    "        with torch.no_grad():\n",
    "            # bootstrap at end of rollout\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            \n",
    "        #Change to GAE when implemented\n",
    "        advantages = compute_advantage_estimates(rewards, values, dones, next_value, next_done)\n",
    "        #advantages = compute_gae(rewards, values, dones, next_value, next_done, 0.98)\n",
    "        returns = advantages + values\n",
    "        advantages.to(device)\n",
    "        returns.to(device)\n",
    "\n",
    "        # flatten the batch\n",
    "        b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # optimize the policy and value network\n",
    "        b_inds = np.arange(hypp.batch_size)\n",
    "        for epoch in range(hypp.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, hypp.batch_size, hypp.minibatch_size):\n",
    "                end = start + hypp.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if hypp.norm_adv:\n",
    "                    mb_advantages = normalize_advantages(mb_advantages)\n",
    "\n",
    "                _, newlogprob, newentropy, newvalue = agent.get_action_and_value(b_observations[mb_inds], b_actions.long()[mb_inds])\n",
    "\n",
    "                # computation of policy objective\n",
    "                pg_loss = compute_policy_objective(mb_advantages, b_logprobs[mb_inds], newlogprob)\n",
    "\n",
    "                # entropy of probability distribution over actions\n",
    "                entropy = newentropy.mean()\n",
    "                                \n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "\n",
    "                if hypp.clip_vloss:\n",
    "                    v_loss = compute_clipped_value_loss(b_returns[mb_inds], b_values[mb_inds], newvalue)\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds])).mean()\n",
    "                loss = pg_loss + hypp.vf_coef * v_loss + hypp.ent_coef * entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # clip gradients\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), hypp.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                gradient_step += 1\n",
    "\n",
    "        # Calculating explained variance metric\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # log losses to tensorboard summary writer\n",
    "        writer.add_scalar(\"hyperparameters/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"train/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"train/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"train/entropy\", entropy.item(), global_step)\n",
    "        writer.add_scalar(\"train/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "        writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "        writer.add_scalar(\"others/EPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    # -------------------------- EVALUATION - DO NOT EDIT ----------------------- #\n",
    "\n",
    "    # one last evaluation stage\n",
    "    if exp.eval_agent:\n",
    "        last_evaluated_episode = episode_step\n",
    "        tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)\n",
    "        print(np.mean(tracked_return))\n",
    "        tracked_returns_over_training.append(tracked_return)\n",
    "        tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "        tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "        # if there has been improvement of the model -\n",
    "        if np.mean(tracked_return) > eval_max_return:\n",
    "            eval_max_return = np.mean(tracked_return)\n",
    "            # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "            hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path = True)\n",
    "\n",
    "        hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)\n",
    "\n",
    "    # ------------------------------- END EVALUATION ---------------------------- #\n",
    "\n",
    "    # ----------------------- CLEANING/SAVING - DO NOT EDIT --------------------- #\n",
    "    envs.close()\n",
    "    envs_eval.close()\n",
    "    writer.close()\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish(quiet=True)\n",
    "        wandb.init(mode=\"disabled\")\n",
    "\n",
    "    hf.save_train_config_to_yaml(exp, hypp)\n",
    "    # ---------------------------- END CLEANING/SAVING -------------------------- #\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe3f0a-060d-4c9e-875d-20a58541df43",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ef678bda1fc6e694726207e9f03336e",
     "grade": false,
     "grade_id": "cell-67f430fb91cf0c24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Your Final Agent (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbdb60-8539-47e0-ba30-5c91daca8404",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f77fe8cd19ad3d2647d1d2df4ae1f52",
     "grade": false,
     "grade_id": "cell-ca3669d22427759a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We assume that the agent you want to submit is stored at `logs/<agent_name_final>/agent_model.pt`.\n",
    "Please write down the name of the final agent you wish to submit.\n",
    "If the agent file is not located at this path, you will not receive points for the performance of your agent—only for the mistakes you have fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b698e-ac8d-4b65-8c2b-1a6213f15908",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9ade969c528e1f3ec1b22b7f7d225f1",
     "grade": false,
     "grade_id": "cell-420b4648d5e0003f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_name_final = \"LunarLander-v2__PPO__42__250602_130406\" #Change this to the name of the final agent you want to submit\n",
    "agent_final = hf.load_model(run_name=agent_name_final)\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.eval_count)])\n",
    "tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent_final, exp.eval_count, exp.seed, greedy_actor=True)\n",
    "mean_reward = np.mean(tracked_return)\n",
    "print(f\"Your final submitted return: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c77c20-50f5-46cd-bd30-1573ac3b5210",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06a2e5b885dde3d8c4d60bba24a538e6",
     "grade": true,
     "grade_id": "cell-b44953495372cbfa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c01fd-43bb-4a14-88e2-68e4eb0220c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "115e6f7de6b045ab192d906f45ce2a93",
     "grade": true,
     "grade_id": "cell-0bffc2a1a6bd9d6c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0e1b0-4e8e-42e8-a374-20555434c305",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edeb151ef1cacecb25f5ff107aef41a1",
     "grade": false,
     "grade_id": "cell-3f86153f40756961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e707c73-01d2-4124-9d59-c4eba9cf8e0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e25721597ebadc13b4745f868cda024",
     "grade": false,
     "grade_id": "cell-51b4f20db31b977c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Display Trained Agent Behaviour\n",
    "\n",
    "Set `agent_name` and `agent_exp_type` to load the saved agent model in the respective log folder and generate a video of the agent's interaction with the gym environment. After the cell is executed, you should see a video embedding as output, and the video is also available in the directory: `/logs/<exp.exp_type>/<exp.run_name>/videos` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fa8fd-81b9-41c7-bd80-bff8564fa5c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "660c3101db6ca614cb6d293861f52df8",
     "grade": false,
     "grade_id": "cell-314c1f63b096afd2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not _grading_mode: #DO NOT EDIT\n",
    "    #-------------From here you can edit-------------\n",
    "    agent_name = exp.run_name\n",
    "    agent_exp_type = exp.exp_type  # both are needed to identify the agent location\n",
    "\n",
    "\n",
    "    exp_folder = \"\" if agent_exp_type is None else agent_exp_type\n",
    "    filepath, _ = hf.create_folder_relative(f\"{exp_folder}/{agent_name}/videos\")\n",
    "\n",
    "    hf.record_video(exp.env_id, agent_name, f\"{filepath}/best.mp4\", exp_type=agent_exp_type, greedy=True)\n",
    "    Video(data=f\"{filepath}/best.mp4\", html_attributes='loop autoplay', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d74433-93b0-4740-b896-413430313077",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba4c2dceb238d202e728d7bde16a94f4",
     "grade": false,
     "grade_id": "cell-40025d552b7d8e49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Performance of Agent(s) during Training\n",
    "\n",
    "During the training loop, if `exp.eval_agent = True`, the performance progress of the agent during its training is saved in a csv file. To compare the saved progress of different agents, create a `dict` containing the parent folder's name of each of the csv files and use the helper function `plotter_agents_training_stats`.\n",
    "\n",
    "To load the data, you can either set `eval_params.run_name00 = exp.run_name` (if only a `tracked_performance_training.csv` file for the corresponding `exp.run_name` exists) or manually enter the folder name containing the csv file. \n",
    "\n",
    "If the agent performance you want to load is inside an exp_type folder, set `eval_params.exp_type00` to experiment type, and if not, set it to `None`. \n",
    "\n",
    "You can add more than one experiment by initializing dict keys and values of the format `eval_params.run_namexx` and `eval_params.exp_typexx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8aafa6-3b68-4f4b-96d9-4eb9b6fc5b45",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "436c346313184c27213d8a4f83d47a49",
     "grade": false,
     "grade_id": "cell-65ebed2d8058b945",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not _grading_mode: #DO NOT EDIT\n",
    "    #-------------From here you can edit-------------\n",
    "    eval_params = edict()  # eval_params - evaluation settings for trained agent\n",
    "\n",
    "    eval_params.run_name00 = exp.run_name\n",
    "    eval_params.exp_type00 = exp.exp_type\n",
    "\n",
    "    agent_labels = []\n",
    "\n",
    "    episode_axis_limit = None\n",
    "\n",
    "    hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14a9cd-b133-4ee3-bc08-1fe333ed8ff3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10f68d4c7766844f6291b83503fdfbf0",
     "grade": false,
     "grade_id": "cell-871343a29e88e924",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## TensorBoard Inline\n",
    "\n",
    "Run the following lines to start-up Tensorboard. It will be displayed in an inline style. Alternatively, you can open a new tab in the browser and go to *localhost:6006* after start-up. If you use Tensorboard through this notebook, then you have to make sure that you terminate the Jupyter server by \"Ctrl+C\" once your session is finished. If not done in this way, you might run into problems starting it up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176c1fa-147c-401d-bb88-0d05a04d4933",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42bcf8b13283412773f2fd2c454308d9",
     "grade": false,
     "grade_id": "cell-8afe5a800bcc85e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b38cc-345b-4013-be01-c6921ff9f5f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d5fd16772a3bc57ca1aa57d7b028f2",
     "grade": true,
     "grade_id": "cell-65f47ed7622642d2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5f468-eb5a-4c1d-94b7-b3f16986fe77",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fb3c258893b07a7026ed8a17e6ea3d0",
     "grade": true,
     "grade_id": "cell-001ccf1bbf378cfc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829a293-535f-4998-a9fb-e2d13d6ad105",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3108bcbaf8444136516b29d0de2aed9a",
     "grade": true,
     "grade_id": "cell-a69d631f24b644cb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5aaff-fdea-45b6-ab50-ddf1bcea9ace",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea5ce915c84a44e417c0da95de6917d8",
     "grade": true,
     "grade_id": "cell-1fce043db29537ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
